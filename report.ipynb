{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer/**GPT**/BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть задачи: Запрогать с нуля одну из моделей. Попробовать немного поучить.\n",
    "\n",
    "Максимальное количество людей: 1\n",
    "\n",
    "Датасет: Любой с текстами, как вариант, wikitext 103\n",
    "\n",
    "Сложность: Средняя\n",
    "\n",
    "Ориентировочное время исполнения: Неделя\n",
    "\n",
    "Метрики: Perplexity\n",
    "\n",
    "Статьи в помощь: Оригинальные, GitHub - karpathy/minGPT: A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training\n",
    "\n",
    "Примерное решение: Просто запрогать модель, читалку данных, обучение и валидацию\n",
    "\n",
    "Критерий выполнения: Перплексия ниже хотя бы 75, адекватные предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание задачи: языковое моделирование, предсказание следующего токена в тексте (с последующим использованием полученных векторных представлений для решения других nlp-задач, но это уже за рамками моего проекта).\n",
    "\n",
    "Формат данных: bpe-токенизированные тексты.\n",
    "\n",
    "Таргет: следующий bpe-токен последовательности.\n",
    "\n",
    "Метрика: Perplexity.\n",
    "\n",
    "Краткий обзор задачи. До появления языковых моделей на основе трансформеров (в частности, [GPT](http://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)) задача решалась рекуррентными сетями. Например:\n",
    "\n",
    "[A. M. Dai and Q. V. Le. Semi-supervised sequence learning](https://arxiv.org/abs/1511.01432)\n",
    "\n",
    "[J. Howard and S. Ruder.  Universal language model fine-tuning for text classification](https://www.aclweb.org/anthology/P18-1031.pdf)\n",
    "\n",
    "Бейзлайн: его не будет. Можно было бы сделать бейзлайн с рекуррентной сетью, но она не очень-то много общего с имеет с трансформерами.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отчет о проделанной работе. \n",
    "\n",
    "***Что было сделано каждым из членов команды? Если вы были один, то пишите только про себя. C какими трудностями вы столкнулись?***\n",
    "\n",
    "Столкнулась с тем, что я ничего не понимаю в оптимизации. И ещё с тем, что на валидации всё было плохо. \n",
    "\n",
    "Почему? В датасете был фанфикшн, примерно 200мб, > 1_000 текстов, в каждом > 20_000 слов. Из-за того, что модель гигантская, пришлось взять для обучения 1/100 датасета. Не пошаффлила всё вместе, поэтому модель, возможно, переобучилась на первых текстах.\n",
    "\n",
    "Самыми фрустрирующими были моменты, когда я не понимала, почему то или иное решение было принято в архитектуре. Например, сначала я применяла LayerNorm к выходу аттеншена, как в статье про GPT1. А потом поменяла (как в статье про GPT2), потому что очень плохо обучалось.\n",
    "\n",
    "***Что было легко/тяжело, что нового узнали?***\n",
    "\n",
    "Тяжело было представить, как трансформируются размерности батча в аттеншен-слое, чтобы параллельно одну и ту же операцию производить на всех головах. То есть до сих пор не получилось представить.\n",
    "\n",
    "Больше всего нового узнала просто про возможности торча и синтаксис торча - всякие мелочи типа .masked_fill и прочего\n",
    "\n",
    "***Каких результатов удалось достичь?***\n",
    "\n",
    "Текст генерируется. Иногда похожий на фанфикшен, потому что склеиваются куски из нескольких токенов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
