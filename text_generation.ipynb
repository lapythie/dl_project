{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import youtokentome as yttm\n",
    "from pprint import pprint\n",
    "from notGPT.model import Config, GPT\n",
    "from notGPT.train import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('GPT_project.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁W', 'uzz', 'up', '▁Beelzebub']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_path = \"pretrained_bpe.model\"\n",
    "tokenizer = yttm.BPE(model=tokenizer_path)\n",
    "tokenizer.encode(\"Wuzzup Beelzebub\", output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available(): device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "config = Config()\n",
    "model = GPT(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict=torch.load(\"last_GPT_model_state_dict.pth\", \n",
    "#                                             map_location=torch.device('cpu')\n",
    "                                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно в данных искать предсказанные куски, но также эти куски гуглятся\n",
    "# with open(data_path, \"r\", encoding=\"utf-8\") as file_obj:\n",
    "#     texts = file_obj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a good thing that Aziraphale knew he was good and he needed to commit '\n",
      " 'to his rage. He was sitting at a stack of hot chocolate beside']\n"
     ]
    }
   ],
   "source": [
    "x = \"It\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=28, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beelzebub had been the former king didn’t even have the time to live happily '\n",
      " 'ever fit with him. However, sometimes that was a part of him. It was no']\n"
     ]
    }
   ],
   "source": [
    "x = \"Beelzebub\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=28, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It may help to understand as well be polite as Aziraphale was nothing he '\n",
      " 'loved by them dangerous and he loved humans, he loved humans, he loved '\n",
      " 'antique shops, he loved talking about']\n"
     ]
    }
   ],
   "source": [
    "x = \"It may help to understand\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=28, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It may help to understand human affairs to be clear that most of the great '\n",
      " 'triumphs and tragedies of history are caused, not by people being '\n",
      " 'fundamentally good or fundamentally bad, but by people being fundamentally '\n",
      " \"people to donas, parents who didn't argue to where she was doing for him to \"\n",
      " 'use her) and to make sure to look her happy, was deeply in any other way '\n",
      " 'than her best knowledge of the area. “Ezekiel, I said not a case she was '\n",
      " 'growing up. “He’s still from the supernatural to find of it.\" \"I don\\'t '\n",
      " \"know, I don't know, for a counter charge or being a fire and we'll pretend \"\n",
      " 'anyone around those days. You are not a messy head.\" He beamed at her. \"You '\n",
      " 'are a son of the quickly-cooling treat. \"Mmm,\" she said, closing her eyes '\n",
      " 'and savoring the taste. \"That is some damn good pie.\" Castiel looked '\n",
      " 'inordinately pleased with himself.']\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"It may help to understand human affairs to be clear that most of the great triumphs \n",
    "and tragedies of history are caused, not by people being fundamentally good or fundamentally bad, \n",
    "but by people being fundamentally people\"\"\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=128, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He had heard about talking to plants in the early seventies, on Radio Four '\n",
      " 'Horsemen of the park.\\xa0 A apple, of yours had gone ignored it, as far as '\n",
      " 'Crowley had applied that no, it was somehow managed to do. If only thing the '\n",
      " 'angel had wanted to do something, he bit of an early age, it as Crowley had '\n",
      " 'wanted to hide back into his way. He had to look at the angel, and of course '\n",
      " 'it was to see him. There was no one of him in his heart to help but the '\n",
      " 'north was no one to see his half-scaled face, his protruding snout. That did '\n",
      " 'nothing to calm the nerves, but he managed to get the door open regardless. '\n",
      " 'Once he was inside, he slammed the door shut and locked everything he’d ever '\n",
      " 'put']\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"He had heard about talking to plants in the early seventies, \n",
    "on Radio Four\"\"\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=128, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You don't have to test everything to destruction just to see if you made it \"\n",
      " 'right a chance, they are very important,\" he said pleadingly. \"Bobby, come '\n",
      " \"on, talk some sense into him! If it's a way to save the world we have to \"\n",
      " 'take it! Right?\" Bobby didn\\'t say anything. \"Right?\" Bobby looked torn. \"It '\n",
      " 'would be better if you just told us all what this miracle plan is,\" he '\n",
      " 'finally said. Sam looked like he was close to losing it. \"I can\\'t!\" he '\n",
      " 'yelled. \"I wish I could but if you knew…\" \"You wouldn\\'t agree to it,\" '\n",
      " 'Castiel said in a low, rough voice. \"None of us would. I\\'m not the only one '\n",
      " 'thinking with my heart here, Sam, but you seem to be the only one willing to '\n",
      " 'sacrifice everything for a slim']\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"You don't have to test everything to destruction just to see if you made it right\"\"\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=128, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you want to imagine the future, imagine a boy and his dog didn’t give for '\n",
      " 'turning a proper thing he would only read when he had entered his own '\n",
      " 'long-lost sword, it had felt like his hands remembered how to grasp the '\n",
      " 'weapon, how to light it afire, and how to direct those energies in the '\n",
      " 'general direction of their foes.\\xa0 Crowley, he noted, didn’t have the '\n",
      " 'benefit of remembering how to use something he had never touched before. He '\n",
      " 'had a few centuries of sporadic martial experience, but all with wholly '\n",
      " 'mortal instruments.\\xa0 Aziraphale yawned, but only out of habit. Angels '\n",
      " 'didn’t really need to sleep, the same way they weren’t beholden to the human '\n",
      " 'instincts to eat, drink, or fornicate. Unless they wanted to, he amended. '\n",
      " 'Even Gabriel']\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"If you want to imagine the future, imagine a boy and his dog\"\"\" \n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=128, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It\\'s like you said the other day,\" said Adam. \"You grow up readin\\' about '\n",
      " \"pirates and cowboys and spacemen and stuff, and jus' when you think the \"\n",
      " \"world's full of amazin' things, they tell you it's really all dead whales \"\n",
      " \"and chopped-down forests and nucular waste hangin' about for millions of \"\n",
      " \"years. 'Snot worth growin' up for, if you ask my opinion on fire), \"\n",
      " 'always.\" She nodded. \"Okay.\" \"Hey,\" he said, before she could disappear with '\n",
      " 'her shame and embarrassment. \"Just what stupid thing are you planning on '\n",
      " 'doing?\" She looked at him. \"What?\" Gabriel gave a shadow of his usual smirk. '\n",
      " '\"No one busts in near-hysterical like that, then attempts to rip someone\\'s '\n",
      " 'clothes off with their teeth, unless something is going down. Something big. '\n",
      " \"Something's eating you up from the inside, Eli, and you're doing your \"\n",
      " 'damndest to push it away and ignore it. So what gives?\" \"I\\'d focus on your '\n",
      " 'own problems if I were you,\" she said, deflecting the question. He raised an '\n",
      " 'eyebrow. \"Meaning?\" \"How did you create a person,']\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"It's like you said the other day,\" said Adam. \"You grow up readin' about pirates \n",
    "and cowboys and spacemen and stuff, and jus' when you think the world's full of amazin' things, \n",
    "they tell you it's really all dead whales and chopped-down forests and nucular waste \n",
    "hangin' about for millions of years. 'Snot worth growin' up for, if you ask my opinion\"\"\"\n",
    "x = tokenizer.encode([x], output_type=yttm.OutputType.ID)\n",
    "x = torch.tensor(x)\n",
    "x = generate(model, x, steps=128, sample=False)\n",
    "pprint( tokenizer.decode(x.detach().tolist()) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
